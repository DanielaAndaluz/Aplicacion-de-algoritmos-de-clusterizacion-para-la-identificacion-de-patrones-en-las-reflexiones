{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos directorio\n",
    "os.chdir('C:\\\\Users\\\\80214178\\\\Documents\\\\Plantillas')\n",
    "#Leemos lista de stopwords\n",
    "stopwordlist=list(pd.read_csv('list.txt', sep=\",\", header=None,encoding='latin-1')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leemos y limpiamos archivos word y excel\n",
    "a=\"Problematización reflexiva\"\n",
    "b=\"Información de la secuencia\"\n",
    "\n",
    "DATA_word=[]\n",
    "for file in glob.glob('*.docx'):\n",
    "    extracto = docx2txt.process(file)\n",
    "    extracto=re.sub('[’:°;,—.\"”“·/!?¿@–#$()%¡=&-><]', ' ', re.sub(r'[0-9]+', ' ', re.sub('\\n', ' ', extracto[extracto.find(a)+26:extracto.find(b)].lower() )))\n",
    "    DATA_word.append(extracto)\n",
    "    \n",
    "DATA_csv=[]\n",
    "for y in glob.glob('*.csv'):\n",
    "    datos =re.sub('[’:;,.\"”“/!?¿@–#$—()%¡=&-><]', '', re.sub(r'[0-9]+', ' ', re.sub('\\n', ' ', pd.read_csv(glob.glob('*.csv')[2]).iloc[5,0].lower())))\n",
    "    DATA_csv.append(datos)\n",
    "#Juntamos\n",
    "DATA=DATA_word+DATA_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizamos\n",
    "DATA_TOK=[]\n",
    "for x in DATA:\n",
    "    token=word_tokenize(x,\"spanish\")\n",
    "    DATA_TOK.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitamos stopwords\n",
    "DATA_TOK_STOP=[]\n",
    "for y in DATA_TOK:\n",
    "    stop = [word for word in y if not word in stopwordlist]\n",
    "    DATA_TOK_STOP.append(stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derivamos\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "DATA_TOK_STOP_STEM=[]\n",
    "for x in DATA_TOK_STOP:\n",
    "    stemmed_spanish = [stemmer.stem(item) for item in x]\n",
    "    DATA_TOK_STOP_STEM.append(stemmed_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words Method\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=dummy,preprocessor=dummy) \n",
    "X=tfidf.fit_transform(DATA_TOK_STOP_STEM).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame (X, columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).to_csv(\"C:\\\\Users\\\\80214178\\\\Documents\\\\X_tfidf.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
